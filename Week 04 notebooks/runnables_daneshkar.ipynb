{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain_openai"
      ],
      "metadata": {
        "id": "Y7eAjyn7cHi4"
      },
      "id": "Y7eAjyn7cHi4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9ac479ee",
      "metadata": {
        "id": "9ac479ee"
      },
      "source": [
        "## Setting Up the Environment\n",
        "\n",
        "Before running the examples, you need to install LangChain. You can install it with the following command:\n",
        "\n",
        "```bash\n",
        "pip install langchain\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RqFfhPTxRoS-"
      },
      "id": "RqFfhPTxRoS-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runnable Interface\n",
        "The Runnable interface in Langchain is a standard interface that many Langchain components adhere to, including chat models, LLMs, output parsers, and more.\n",
        "\n",
        "### Key Features of the Runnable Protocol\n",
        "- **Standard Interface**: The Runnable protocol provides a consistent way to interact with different components.\n",
        "- **Core Methods**:\n",
        "LangChain supports several methods for running chains, including `invoke`, `stream`, `batch`, and their asynchronous counterparts.\n",
        "\n",
        "| Method    | Description                                                                 |\n",
        "|-----------|-----------------------------------------------------------------------------|\n",
        "| `invoke`  | Calls the runnable on a single input, providing immediate results.             |\n",
        "| `stream`  | Streams back chunks of the response, allowing for real-time data processing.|\n",
        "| `batch`   | Processes a list of inputs in one go, optimizing performance.               |\n",
        "| `ainvoke` | Invokes the runnable on an input asynchronously.                               |\n",
        "| `astream` | Streams back chunks of the response asynchronously.                        |\n",
        "| `abatch`  | Handles a list of inputs asynchronously.                                    |\n",
        "\n"
      ],
      "metadata": {
        "id": "HmIWEGHgRovn"
      },
      "id": "HmIWEGHgRovn"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXUzuSzDT7pC"
      },
      "id": "fXUzuSzDT7pC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Input and output types**\n",
        "Every `Runnable` is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.\n",
        "\n",
        "Runnable methods that result in the execution of the Runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) work with these input and output types:\n",
        "\n",
        "- `invoke`: Accepts an input and returns an output.\n",
        "- `batch`: Accepts a list of inputs and returns a list of outputs.\n",
        "- `stream`: Accepts an input and returns a generator that yields outputs.\n",
        "\n",
        "The input type and output type vary by component:\n",
        "\n",
        "| Component     | Input Type                                      | Output Type             |\n",
        "|---------------|-------------------------------------------------|-------------------------|\n",
        "| Prompt        | dictionary                                      | PromptValue             |\n",
        "| ChatModel     | a string, list of chat messages, or a PromptValue | ChatMessage             |\n",
        "| LLM           | a string, list of chat messages, or a PromptValue | String                  |\n",
        "| OutputParser  | the output of an LLM or ChatModel                | Depends on the parser   |\n",
        "| Retriever     | a string                                         | List of Documents       |\n",
        "| Tool          | a string or dictionary, depending on the tool    | Depends on the tool     |\n"
      ],
      "metadata": {
        "id": "J7AnsEfaT8Gm"
      },
      "id": "J7AnsEfaT8Gm"
    },
    {
      "cell_type": "markdown",
      "id": "694b53b3",
      "metadata": {
        "id": "694b53b3"
      },
      "source": [
        "## Practical Examples\n",
        "\n",
        "### 1. Invoking a Runnable\n",
        "\n",
        "Let's create a simple example where we use a RunnableLambda to invoke a function.\n",
        "\n",
        "```python\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Define a simple function\n",
        "runnable = RunnableLambda(lambda x: f'Hello {x}')\n",
        "\n",
        "# Invoke the function\n",
        "result = runnable.invoke('World')\n",
        "print(result)  # Expected output: 'Hello World'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Explanation:\n",
        "- **Post-Processing**: The `post_process` function removes unwanted code block delimiters (e.g., ```python, ```persian), as well as Persian and English-specific tags.\n",
        "- **RunnableLambda**: We use `RunnableLambda` to turn the `post_process` function into a runnable that we can invoke with the LLM's raw output.\n",
        "- **Output**: The unwanted code block delimiters are removed, and the result is cleaner and more user-friendly.\n",
        "\n"
      ],
      "metadata": {
        "id": "YYyqpLIDbUoy"
      },
      "id": "YYyqpLIDbUoy"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Define the post-processing function to clean up code snippets\n",
        "def post_process(text):\n",
        "    return text.replace('```python', '')\\\n",
        "        .replace('```text', '')\\\n",
        "        .replace('```', '')\\\n",
        "        .replace('```', '')  # Removing extra code block markers\n",
        "\n",
        "# Define a RunnableLambda with the post-process function\n",
        "runnable = RunnableLambda(lambda x: post_process(x))\n",
        "\n",
        "# Simulated LLM response with unwanted formatting\n",
        "llm_response = \"\"\"\n",
        "```python\n",
        "def greet(name):\n",
        "    print(f\"Hello, {name}!\")\n",
        "    ```\"\"\"\n",
        "\n",
        "\n",
        "runnable.invoke(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SkEPpP5WbT77",
        "outputId": "99eaaca4-2bad-4e74-e50a-e85f44a52522"
      },
      "id": "SkEPpP5WbT77",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\ndef greet(name):\\n    print(f\"Hello, {name}!\")\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tAiVBMi2dLlw"
      },
      "id": "tAiVBMi2dLlw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synchronous vs Asynchronous Programming\n",
        "\n",
        "### Synchronous Programming:\n",
        "In synchronous programming, tasks are executed one after the other, in a sequence. This means that the program waits for each task to finish before moving on to the next one. If a task takes time (such as an API call or reading from a file), the entire program halts until that task is completed.\n",
        "\n",
        "**Example:**\n",
        "\n"
      ],
      "metadata": {
        "id": "kT5H2uvod5Hl"
      },
      "id": "kT5H2uvod5Hl"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def task1():\n",
        "    print(\"Task 1 started\")\n",
        "    # Simulate a time-consuming task\n",
        "    time.sleep(2)\n",
        "    print(\"Task 1 finished\")\n",
        "\n",
        "def task2():\n",
        "    print(\"Task 2 started\")\n",
        "    time.sleep(1)\n",
        "    print(\"Task 2 finished\")\n",
        "\n",
        "# Synchronous execution\n",
        "task1()\n",
        "task2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSDzSVemd4n7",
        "outputId": "ab587547-2b60-4410-f8f2-7c5c8fca61f1"
      },
      "id": "gSDzSVemd4n7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1 started\n",
            "Task 1 finished\n",
            "Task 2 started\n",
            "Task 2 finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JN0WyDVUeMn5"
      },
      "id": "JN0WyDVUeMn5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asynchronous Programming:\n",
        "\n",
        "\n",
        "In asynchronous programming, tasks are executed concurrently, meaning the program can start a task and move on to others without waiting for the first one to finish. When a task is done, it notifies the program. This allows for better performance, especially when dealing with time-consuming operations.\n",
        "\n",
        "## Why Asynchronous Programming is Important\n",
        "\n",
        "Asynchronous programming allows you to execute multiple operations without blocking the main thread. This is particularly useful when dealing with long-running tasks, such as API calls to language models. By leveraging async capabilities, developers can manage multiple tasks concurrently, allowing for smoother user experiences and reduced wait times.\n",
        "\n",
        "### Benefits of Asynchronous Programming:\n",
        "\n",
        "- **Improved Performance**: Enables parallel execution of tasks, improving overall application performance.\n",
        "- **Responsive User Interfaces**: Keeps the UI interactive during time-consuming operations.\n",
        "- **Scalability**: Handles more users and requests without significant delays.\n",
        "- **Better Resource Utilization**: Async chains can help in making better use of system resources, especially in applications that require high throughput.\n",
        "\n",
        "\n",
        "\n",
        "**Example**:"
      ],
      "metadata": {
        "id": "grMXj0ibeNEv"
      },
      "id": "grMXj0ibeNEv"
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def task1():\n",
        "    print(\"Task 1 started\")\n",
        "    await asyncio.sleep(2)\n",
        "    print(\"Task 1 finished\")\n",
        "\n",
        "async def task2():\n",
        "    print(\"Task 2 started\")\n",
        "    await asyncio.sleep(1)\n",
        "    print(\"Task 2 finished\")\n",
        "\n",
        "# Asynchronous execution\n",
        "async def main():\n",
        "    await asyncio.gather(task1(), task2())\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pn5hs-leVni",
        "outputId": "c5b5dfe3-2c7b-41fd-c394-4d781bacf442"
      },
      "id": "3pn5hs-leVni",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1 started\n",
            "Task 2 started\n",
            "Task 2 finished\n",
            "Task 1 finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How to call a runnable asynchronously?**\n",
        "\n"
      ],
      "metadata": {
        "id": "Vw--f8R7djV8"
      },
      "id": "Vw--f8R7djV8"
    },
    {
      "cell_type": "code",
      "source": [
        "# await some_runnable.ainvoke(some_input)\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Define the post-processing function to clean up code snippets\n",
        "def post_process(text):\n",
        "    return text.replace('```python', '')\\\n",
        "        .replace('```text', '')\\\n",
        "        .replace('```', '')\\\n",
        "        .replace('```', '')  # Removing extra code block markers\n",
        "\n",
        "# Define a RunnableLambda with the post-process function\n",
        "runnable = RunnableLambda(lambda x: post_process(x))\n",
        "\n",
        "# Simulated LLM response with unwanted formatting\n",
        "llm_response = \"\"\"\n",
        "```python\n",
        "def greet(name):\n",
        "    print(f\"Hello, {name}!\")\n",
        "    ```\"\"\"\n",
        "\n",
        "\n",
        "await runnable.ainvoke(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nDA3HRuXnVEw",
        "outputId": "9224b416-681b-4cff-f2b5-8129adf6d9f3"
      },
      "id": "nDA3HRuXnVEw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\ndef greet(name):\\n    print(f\"Hello, {name}!\")\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f1436e2",
      "metadata": {
        "id": "6f1436e2"
      },
      "source": [
        "## **When to Use Batch Processing and Its Benefits**\n",
        "\n",
        "### Overview\n",
        "\n",
        "Batch processing is ideal for jobs that do not require immediate responses. you can process a group of requests asynchronously, offering several benefits over synchronous API calls. This method is particularly useful for use cases that involve large datasets or require high throughput, but can afford a delay in processing.\n",
        "\n",
        "### Use Cases for Batch Processing\n",
        "Batch processing is often used in scenarios like:\n",
        "\n",
        "- **Running evaluations**: Processing multiple models or tasks that require evaluation.\n",
        "- **Classifying large datasets**: Handling large volumes of data that need to be categorized or labeled.\n",
        "- **Embedding content repositories**: Creating embeddings for large collections of documents or data.\n",
        "\n",
        "### When to Use the Batch API\n",
        "\n",
        "- **Non-Urgent Tasks**: When your job doesn't need an immediate response but you want to process a large amount of data efficiently.\n",
        "- **High Throughput**: When you need to handle large volumes of requests but are limited by rate limits or API costs in a synchronous setup.\n",
        "- **Cost-Effective Solutions**: When working with large datasets or evaluations, batch processing helps save significantly on API costs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "You can also process multiple inputs at once using `Runnable.batch()`.\n",
        "\n",
        "```python\n",
        "\n",
        "# Define a simple batch function\n",
        "runnable = RunnableLambda(lambda x: x * 2)\n",
        "\n",
        "# Use batch processing\n",
        "results = runnable.batch([1, 2, 3, 4])\n",
        "print(results)  # Expected output: [2, 4, 6, 8]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runnable = RunnableLambda(lambda x: x * 2)\n",
        "\n",
        "# Use batch processing\n",
        "results = runnable.batch([1, 2, 3, 4])\n",
        "print(results)  # Expected output: [2, 4, 6, 8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnR83zhEnLlZ",
        "outputId": "f2ebf7f9-d4a8-4def-8f37-deb0e6b30623"
      },
      "id": "pnR83zhEnLlZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e0cb8b",
      "metadata": {
        "id": "62e0cb8b"
      },
      "source": [
        "## Stream a Runnable\n",
        "\n",
        "`Runnable.stream()` and `Runnable.astream()` are useful for cases where you want to process and retrieve data incrementally or in chunks, without waiting for the entire operation to complete. This is particularly beneficial when you're working with large datasets, long-running tasks, or making repeated calls to external services like LLMs.\n",
        "\n",
        "### Use Case for Streaming\n",
        "\n",
        "Streaming is particularly meaningful when you're working with LLMs (Language Models) and you need to process the results as they are generated, especially for long texts or when multiple API calls are required. Instead of waiting for the entire response to be generated, you can stream the output incrementally, which allows for real-time processing and more efficient use of system resources.\n",
        "\n",
        "For example, consider a scenario where you need to generate a long text from an LLM that involves multiple steps or calls. Instead of waiting for the entire output to be returned, you can process the output piece by piece as the model generates it.\n",
        "\n",
        "### Code Example: Streaming with an LLM Request\n",
        "\n",
        "Here’s how you might use `Runnable.stream()` to process the output of an LLM incrementally:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# A function that simulates a long-running LLM call and streams the response\n",
        "def generate_text_from_llm(prompt):\n",
        "    # Simulate streaming the response by yielding parts of the output\n",
        "    parts = [\n",
        "        \"Once upon a time, \",\n",
        "        \"there was a brave knight who ventured into the forest. \",\n",
        "        \"Along the way, he encountered many challenges, \"\n",
        "        \"but he always remained determined. \"\n",
        "    ]\n",
        "\n",
        "    for part in parts:\n",
        "        yield part  # Yield parts of the text incrementally\n",
        "\n",
        "# Create a RunnableLambda that wraps our function\n",
        "runnable = RunnableLambda(generate_text_from_llm)\n",
        "\n",
        "# Simulate generating a long text using the LLM\n",
        "prompt = \"Tell me a story about a brave knight.\"\n",
        "\n",
        "# Stream and print each part of the text incrementally\n",
        "for chunk in runnable.stream(prompt):\n",
        "    print(chunk)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4_zeGbbqoqx",
        "outputId": "837a7cb0-ee0c-43cf-c0be-ba37b29a98e7"
      },
      "id": "M4_zeGbbqoqx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, \n",
            "there was a brave knight who ventured into the forest. \n",
            "Along the way, he encountered many challenges, but he always remained determined. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('openai_api_key')\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=openai_api_key,model='gpt-4o' ,temperature=0)\n",
        "prompt = \"Tell me a ploite joke about.at most 20 words\"\n",
        "\n",
        "# Stream and print each part of the text incrementally\n",
        "text = \"\"\n",
        "for chunk in llm.stream(prompt):\n",
        "    text += chunk.content\n",
        "    print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9miB2jnq7T1",
        "outputId": "680ae01e-52e8-4cbb-fae1-a7aacce9dbc2"
      },
      "id": "p9miB2jnq7T1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Why\n",
            "Why did\n",
            "Why did the\n",
            "Why did the scare\n",
            "Why did the scarecrow\n",
            "Why did the scarecrow win\n",
            "Why did the scarecrow win an\n",
            "Why did the scarecrow win an award\n",
            "Why did the scarecrow win an award?\n",
            "Why did the scarecrow win an award? Because\n",
            "Why did the scarecrow win an award? Because he\n",
            "Why did the scarecrow win an award? Because he was\n",
            "Why did the scarecrow win an award? Because he was outstanding\n",
            "Why did the scarecrow win an award? Because he was outstanding in\n",
            "Why did the scarecrow win an award? Because he was outstanding in his\n",
            "Why did the scarecrow win an award? Because he was outstanding in his field\n",
            "Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "Why did the scarecrow win an award? Because he was outstanding in his field!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a6f453",
      "metadata": {
        "id": "d3a6f453"
      },
      "source": [
        "### 4. Composing Runnables\n",
        "\n",
        "You can chain runnables together using the pipe operator `|`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing and postprocessing steps\n",
        "preprocess = RunnableLambda(lambda x: x.lower().strip())\n",
        "postprocess = RunnableLambda(lambda x: f'Processed: {x}')\n",
        "\n",
        "# Chain the runnables\n",
        "pipeline = preprocess | postprocess\n",
        "\n",
        "# Invoke the pipeline\n",
        "result = pipeline.invoke('   Hello World    ')\n",
        "print(result)  # Expected output: 'Processed: hello world'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiEblzhVsG_W",
        "outputId": "7f9f6a30-d2d9-4e63-bad7-3854d13cb322"
      },
      "id": "DiEblzhVsG_W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08b8a32e",
      "metadata": {
        "id": "08b8a32e"
      },
      "source": [
        "### 5. Parallel Execution\n",
        "\n",
        "You can execute tasks in parallel using `RunnableParallel`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
        "\n",
        "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
        "runnable2 = RunnableLambda(lambda x: [x] * 2)\n",
        "\n",
        "chain = RunnableParallel(first=runnable1, second=runnable2)\n",
        "\n",
        "chain.invoke(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78QlpyC0Khfs",
        "outputId": "b4d291d2-00da-4a4d-a434-b9f454888dc9"
      },
      "id": "78QlpyC0Khfs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'first': {'foo': 2}, 'second': [2, 2]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Compose Runnables Using the .pipe method\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "--9wGsKjKiDy"
      },
      "id": "--9wGsKjKiDy"
    },
    {
      "cell_type": "code",
      "source": [
        "runnable1 | runnable2"
      ],
      "metadata": {
        "id": "TOSczW2pdxA7"
      },
      "id": "TOSczW2pdxA7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Define the first runnable that returns a dictionary\n",
        "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
        "\n",
        "# Define the second runnable that processes the output of the first runnable\n",
        "runnable2 = RunnableLambda(lambda x: [x] * 3)\n",
        "\n",
        "# Chain the runnables together using the pipe operator\n",
        "chain = runnable1.pipe(runnable2)\n",
        "\n",
        "# Invoke the composed chain\n",
        "result = chain.invoke(2)\n",
        "\n",
        "# Output the result\n",
        "print(result)  # Expected output: [{'foo': 2}, {'foo': 2}]\n"
      ],
      "metadata": {
        "id": "U5S6AaeGthWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29541d62-698b-4665-cc80-001ac8226fc5"
      },
      "id": "U5S6AaeGthWk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'foo': 2}, {'foo': 2}, {'foo': 2}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc3f590",
      "metadata": {
        "id": "8bc3f590"
      },
      "source": [
        "### 6. Creating Custom Runnables\n",
        "\n",
        "You can turn any function into a runnable using `RunnableLambda`.\n",
        "\n",
        "```python\n",
        "from langchain.runnables import RunnableLambda\n",
        "\n",
        "# Define a custom function\n",
        "custom_runnable = RunnableLambda(lambda x: f'Square: {x ** 2}')\n",
        "\n",
        "# Invoke the custom runnable\n",
        "result = custom_runnable.invoke(5)\n",
        "print(result)  # Expected output: 'Square: 25'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dS90q4xBL-em"
      },
      "id": "dS90q4xBL-em",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Include Input Dictionary in Output Dictionary Using `RunnablePassthrough`\n",
        "\n",
        "In some scenarios, you may want to include the entire input dictionary in the output alongside the results of transformations or computations. The `RunnablePassthrough` method allows you to pass the input data through unchanged while applying other transformations in parallel. This is useful when you want to retain the original input alongside new or modified data in the output.\n",
        "\n",
        "In this example, we use `RunnablePassthrough` to pass the input dictionary through unchanged, while another `RunnableLambda` performs a transformation on the `\"foo\"` key. The `RunnableParallel` is then used to run both tasks in parallel, combining the original input with the new result.\n"
      ],
      "metadata": {
        "id": "mJvDJv9KL_Iu"
      },
      "id": "mJvDJv9KL_Iu"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import (\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "\n",
        "# Define a runnable that adds 7 to the value of the \"foo\" key\n",
        "runnable1 = RunnableLambda(lambda x: x[\"foo\"] + 7)\n",
        "\n",
        "# Use RunnableParallel to run the transformation and passthrough in parallel\n",
        "chain = RunnableParallel(bar=runnable1, baz=RunnablePassthrough())\n",
        "\n",
        "# Invoke the chain with an input dictionary\n",
        "result = chain.invoke({\"foo\": 10})\n",
        "\n",
        "# Output the result\n",
        "print(result)  # Expected output: {'foo': 10, 'bar': 17, 'baz': {'foo': 10}}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g42w25HoLfoi",
        "outputId": "8a8a5b96-3ca2-4c1b-915d-ae6ae649f2b1"
      },
      "id": "g42w25HoLfoi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bar': 17, 'baz': {'foo': 10}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge Input and Output Dictionaries Using `RunnablePassthrough.assign`\n",
        "\n",
        "In many cases, you may want to merge or assign values from the output of one runnable to the input of another. `RunnablePassthrough.assign` provides a way to pass through the input dictionary while applying transformations to specific keys. This is useful when you need to modify only a part of the input data while preserving the rest.\n",
        "\n",
        "In this example, we use `RunnablePassthrough.assign` to merge the input dictionary with the output of a transformation. The transformation function (`runnable1`) adds `7` to the value associated with the `\"foo\"` key in the input dictionary. We then assign the result to a new key, `\"bar\"`, in the output dictionary.\n",
        "\n"
      ],
      "metadata": {
        "id": "HIV1wBKpLf8c"
      },
      "id": "HIV1wBKpLf8c"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "runnable1 = RunnableLambda(lambda x: x[\"foo\"] + 7)\n",
        "\n",
        "chain = RunnablePassthrough.assign(bar=runnable1)\n",
        "\n",
        "chain.invoke({\"foo\": 10})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B90sKnXpLEEe",
        "outputId": "77a5f991-373d-4d06-d406-8693de0bcb69"
      },
      "id": "B90sKnXpLEEe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'foo': 10, 'bar': 17}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e9B16WW7P8KF"
      },
      "id": "e9B16WW7P8KF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}